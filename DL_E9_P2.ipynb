{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Auto encoder**"
      ],
      "metadata": {
        "id": "mMbkUiqM9dmH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QzStF_m_9ZMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39a0339-4893-4769-971d-d5ea0a6cd947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-683616cd98db>:8: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
            "  display.set_matplotlib_formats('svg')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "display.set_matplotlib_formats('svg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Import & Processing the data**"
      ],
      "metadata": {
        "id": "E3ThZH439lv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.loadtxt(open('sample_data/mnist_train_small.csv','rb'),delimiter=',')\n",
        "labels = data[:,0]\n",
        "data = data[:,1:]\n",
        "dataNorm = data / np.max(data)\n",
        "dataT = torch.tensor( dataNorm ).float()"
      ],
      "metadata": {
        "id": "tIy9etUN9jUu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the DL Model**"
      ],
      "metadata": {
        "id": "hG20R3Ty-a1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def createTheMNISTAE():\n",
        "  class aenet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.input = nn.Linear(784,150)\n",
        "      self.enc = nn.Linear(150,15)\n",
        "      self.lat = nn.Linear(15,150)\n",
        "      self.dec = nn.Linear(150,784)\n",
        "    def forward(self,x):\n",
        "      x = F.relu( self.input(x) )\n",
        "      codex = F.relu( self.enc(x) )\n",
        "      x = F.relu( self.lat(codex) )\n",
        "      y = torch.sigmoid( self.dec(x) )\n",
        "      return y,codex\n",
        "  net = aenet()\n",
        "  lossfun = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(net.parameters(),lr=.001) #change lr here\n",
        "  return net,lossfun,optimizer\n",
        "net,lossfun,optimizer = createTheMNISTAE()\n",
        "X = dataT[:5,:]\n",
        "yHat = net(X)\n",
        "print('Input shape:')\n",
        "print(X.shape)\n",
        "print(' ')\n",
        "print(type(yHat),len(yHat))\n",
        "print(' ')\n",
        "print('Shape of model output:')\n",
        "print(yHat[0].shape)\n",
        "print(' ')\n",
        "print('Shape of encoding layer output:')\n",
        "print(yHat[1].shape)"
      ],
      "metadata": {
        "id": "KRuLkh0B9rDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0cd042-bd84-4aaf-cc56-58a286193fee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape:\n",
            "torch.Size([5, 784])\n",
            " \n",
            "<class 'tuple'> 2\n",
            " \n",
            "Shape of model output:\n",
            "torch.Size([5, 784])\n",
            " \n",
            "Shape of encoding layer output:\n",
            "torch.Size([5, 50])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To change the activation function, you can modify the forward method of the aenet class in the createTheMNISTAE function. Here's an example of using the hyperbolic tangent (tanh) activation function instead of the rectified linear unit (ReLU) function:\n",
        "```\n",
        "def createTheMNISTAE():\n",
        "    class aenet(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.input = nn.Linear(784, 150)\n",
        "            self.enc = nn.Linear(150, 15)\n",
        "            self.lat = nn.Linear(15, 150)\n",
        "            self.dec = nn.Linear(150, 784)\n",
        "        def forward(self, x):\n",
        "            x = torch.tanh(self.input(x))\n",
        "            codex = torch.tanh(self.enc(x))\n",
        "            x = torch.tanh(self.lat(codex))\n",
        "            y = torch.sigmoid(self.dec(x))\n",
        "            return y, codex\n",
        "    \n",
        "    net = aenet()\n",
        "    lossfun = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=.001)\n",
        "    return net, lossfun, optimizer\n",
        "\n",
        "net, lossfun, optimizer = createTheMNISTAE()\n",
        "X = dataT[:5, :]\n",
        "yHat = net(X)\n",
        "print('Input shape:')\n",
        "print(X.shape)\n",
        "print(' ')\n",
        "print(type(yHat), len(yHat))\n",
        "print(' ')\n",
        "print('Shape of model output:')\n",
        "print(yHat[0].shape)\n",
        "print(' ')\n",
        "print('Shape of encoding layer output:')\n",
        "print(yHat[1].shape)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "bFxv2tMJ7diH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "class aenet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input = nn.Linear(784, 300)\n",
        "        self.enc = nn.Linear(300, 50)\n",
        "        self.lat = nn.Linear(50, 300)\n",
        "        self.dec = nn.Linear(300, 784)\n",
        "        self.output = nn.Linear(784, 784)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.input(x))\n",
        "        codex = F.relu(self.enc(x))\n",
        "        x = F.relu(self.lat(codex))\n",
        "        y = torch.sigmoid(self.dec(x))\n",
        "        z = torch.sigmoid(self.output(y))\n",
        "        return z, codex\n",
        "\n",
        "\n",
        "In this modification, we added an additional linear layer called self.output with input and output dimensions of 784, the same as the input and output dimensions of the autoencoder. We also changed the input and output dimensions of the first and last linear layers to match the input and output dimensions of self.output. Finally, we modified the forward method to include the additional layer and return its output as well.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "IhZTTzhX6Yue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function2trainTheModel():\n",
        "  numepochs = 10000 #change here\n",
        "  net,lossfun,optimizer = createTheMNISTAE()\n",
        "  losses = torch.zeros(numepochs)\n",
        "  for epochi in range(numepochs):\n",
        "    randomidx = np.random.choice(dataT.shape[0],size=32)\n",
        "    X = dataT[randomidx,:]\n",
        "    yHat = net(X)[0] \n",
        "    loss = lossfun(yHat,X)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses[epochi] = loss.item()\n",
        "  return losses,net"
      ],
      "metadata": {
        "id": "SJXYcLI0-hoS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses,net = function2trainTheModel()\n",
        "print(f'Final loss: {losses[-1]:.4f}')\n",
        "plt.plot(losses,'.-')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Model loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "FflxlrTBAKbW",
        "outputId": "4d24b2db-77f5-4717-a85b-fcd84effe469"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-ea6409627d65>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction2trainTheModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Final loss: {losses[-1]:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-dd08947d7684>\u001b[0m in \u001b[0;36mfunction2trainTheModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myHat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepochi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yHat,latent = net(dataT)\n",
        "print(yHat.shape)\n",
        "print(latent.shape)\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
        "ax[0].hist(latent.flatten().detach(),100)\n",
        "ax[0].set_xlabel('Latent activation value')\n",
        "ax[0].set_ylabel('Count')\n",
        "ax[0].set_title('Distribution of latent units activations')\n",
        "ax[1].imshow(latent.detach(),aspect='auto',vmin=0,vmax=10)\n",
        "ax[1].set_xlabel('Latent node')\n",
        "ax[1].set_ylabel('Image number')\n",
        "ax[1].set_title('All latent activations')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nzjZaM4OAPdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sourcecode = np.zeros((latent.shape[1],10))\n",
        "for i in range(10):\n",
        "  digidx = np.where(labels==i)\n",
        "  sourcecode[:,i] = torch.mean(latent[digidx,:],axis=1).detach()\n",
        "fig = plt.figure(figsize=(8,5))\n",
        "plt.plot(sourcecode,'s-')\n",
        "plt.legend(range(10),loc=(1.01,.4))\n",
        "plt.xticks(range(15))\n",
        "plt.xlabel('Latent node number')\n",
        "plt.ylabel('Activation')\n",
        "plt.title(\"The model's internal representation of the numbers\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0rmGi2z4AR40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pcaData = PCA(n_components=15).fit(data) \n",
        "pcaCode = PCA( ).fit(latent.detach())\n",
        "plt.plot(100*pcaData.explained_variance_ratio_,'s-',label='Data PCA')\n",
        "plt.plot(100*pcaCode.explained_variance_ratio_,'o-',label='Code PCA')\n",
        "plt.xlabel('Components')\n",
        "plt.ylabel('Percent variance explained')\n",
        "plt.title('PCA scree plot')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "scoresData = pcaData.fit_transform(data)\n",
        "scoresCode = pcaCode.fit_transform(latent.detach())\n",
        "fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
        "for lab in range(10):\n",
        "  ax[0].plot(scoresData[labels==lab,0],scoresData[labels==lab,1],'o',markersize=3,alpha=.4)\n",
        "  ax[1].plot(scoresCode[labels==lab,0],scoresCode[labels==lab,1],'o',markersize=3,alpha=.4)\n",
        "for i in range(2):\n",
        "  ax[i].set_xlabel('PC1 projection')\n",
        "  ax[i].set_ylabel('PC2 projection')\n",
        "  ax[i].legend(range(10))\n",
        "ax[0].set_title('PCA of data')\n",
        "ax[1].set_title('PCA of latent code')\n",
        "plt.show()\n",
        "fig,ax = plt.subplots(1,3,figsize=(15,3))\n",
        "ax[0].imshow(dataT[0,:].view(28,28),cmap='gray')\n",
        "ax[1].plot(dataT[0,:],'ks')\n",
        "ax[1].set_xlabel('Pixels (vectorized)')\n",
        "ax[1].set_ylabel('Intensity value')\n",
        "ax[2].plot(latent[0,:].detach(),'ks')\n",
        "ax[2].set_xlabel('Latent units')\n",
        "ax[2].set_ylabel('Activation (a.u.)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l4kiv_G2AViE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}